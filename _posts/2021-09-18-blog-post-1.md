---
title: 'A Layer-wise Probing on BERToidsâ€™ Representations'
date: 2021-09-18
permalink: /posts/layer-wise-probing-on-bertoids/
tags:
  - Natural Language Processing
  - Probing
  - BERToids
---

<img align="right" src="/images/posts/2021-09-layer-wise-probing-on-bertoids/mdl_layers.png" width="250" >
This is a post for the EMNLP 2021 (BlackboxNLP) paper [Not All Models Localize Linguistic Knowledge in the Same Place: A Layer-wise Probing on BERToids' Representations](https://arxiv.org/abs/2109.05958)
<br><br>
In this work, we extend the probing studies to ELECTRA and XLNet, showing that variations in the pre-training objectives can result in different behaviors in encoding linguistic information. We show that
<ul>
  <li>Weight mixing results in edge probing does not lead to reliable conclusions in layer-wise cross model analysis studies and MDL probing is more informative in this setup.</li>
  <li>XLNet accumulates linguistic knowledge in the earlier layers than BERT, whereas that of ELECTRA is in the final layers.</li>
  <li>ELECTRA undergoes a slight change during fine-tuning, whereas XLNet experiences significant adjustments.</li>
</ul>
<a class="btn btn--info" href="/posts/layer-wise-probing-on-bertoids/">âž¤ read more</a>
<a class="btn btn--info" href="https://arxiv.org/abs/2109.05958">ðŸ“„ read paper</a>

---


Aren't headings cool?
------