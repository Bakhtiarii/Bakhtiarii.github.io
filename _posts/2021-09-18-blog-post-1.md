---
title: 'A Layer-wise Probing on BERToids‚Äô Representations'
date: 2021-09-18
permalink: /posts/layer-wise-probing-on-bertoids/
tags:
  - Natural Language Processing
  - Probing
  - BERToids
---

<img align="right" src="/images/posts/2021-09-layer-wise-probing-on-bertoids/mdl_layers.png" width="250" >
This is a post for the EMNLP 2021 (BlackboxNLP) paper [Not All Models Localize Linguistic Knowledge in the Same Place: A Layer-wise Probing on BERToids' Representations](https://arxiv.org/abs/2109.05958)
<br><br>
In this work, we extend the probing studies to ELECTRA and XLNet, showing that variations in the pre-training objectives can result in different behaviors in encoding linguistic information. We show that
* 
<br>
<a class="btn btn--info" style="text-decoration: none" href="https://arxiv.org/abs/2109.05958">üìù Read paper</a>

---

Headings are cool
======

You can have many headings
======

Aren't headings cool?
------